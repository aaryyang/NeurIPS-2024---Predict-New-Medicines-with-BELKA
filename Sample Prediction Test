{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":67356,"databundleVersionId":8006601,"sourceType":"competition"},{"sourceId":8275617,"sourceType":"datasetVersion","datasetId":4914065},{"sourceId":8773525,"sourceType":"datasetVersion","datasetId":5272782},{"sourceId":8819963,"sourceType":"datasetVersion","datasetId":5214038}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-01T07:24:27.169710Z","iopub.execute_input":"2024-07-01T07:24:27.170075Z","iopub.status.idle":"2024-07-01T07:24:41.317679Z","shell.execute_reply.started":"2024-07-01T07:24:27.170045Z","shell.execute_reply":"2024-07-01T07:24:41.316325Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport os\nimport random\nimport joblib\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import average_precision_score as APS\n\n# Configuration class\nclass CFG:\n    PREPROCESS = False\n    PRETRAINED = True\n    EPOCHS = 30\n    BATCH_SIZE = 4096\n    LR = 1e-3\n    WD = 0.05\n    NBR_FOLDS = 5\n    SELECTED_FOLDS = [0, 1, 2, 3, 4]\n    EXIST_MODELS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n    SEED = 2222\n\n# Function to set seeds for reproducibility\ndef set_seeds(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n\nset_seeds(seed=CFG.SEED)\n\n# Encoding dictionary ensuring all values are within the range [0, 35]\nenc = {'l': 1, 'y': 2, '@': 3, '3': 4, 'H': 5, 'S': 6, 'F': 7, 'C': 8, 'r': 9, 's': 10, '/': 11, 'c': 12, 'o': 13,\n       '+': 14, 'I': 15, '5': 16, '(': 17, '2': 18, ')': 19, '9': 20, 'i': 21, '#': 22, '6': 23, '8': 24, '4': 25, '=': 26,\n       '1': 27, 'O': 28, '[': 29, 'D': 30, 'B': 31, ']': 32, 'N': 33, '7': 34, 'n': 35, '-': 0}\n\n# Preprocessing function\nif CFG.PREPROCESS:\n    train_raw = pd.read_parquet('/kaggle/input/leash-BELKA/train.parquet')\n    smiles = train_raw[train_raw['protein_name']=='BRD4']['molecule_smiles'].values\n    assert (smiles!=train_raw[train_raw['protein_name']=='HSA']['molecule_smiles'].values).sum() == 0\n    assert (smiles!=train_raw[train_raw['protein_name']=='sEH']['molecule_smiles'].values).sum() == 0\n    \n    def encode_smile(smile):\n        tmp = [enc[i] for i in smile]\n        tmp = tmp + [0]*(142-len(tmp))\n        return np.array(tmp).astype(np.uint8)\n\n    smiles_enc = joblib.Parallel(n_jobs=96)(joblib.delayed(encode_smile)(smile) for smile in tqdm(smiles))\n    smiles_enc = np.stack(smiles_enc)\n    train = pd.DataFrame(smiles_enc, columns=[f'enc{i}' for i in range(142)])\n    train['bind1'] = train_raw[train_raw['protein_name']=='BRD4']['binds'].values\n    train['bind2'] = train_raw[train_raw['protein_name']=='HSA']['binds'].values\n    train['bind3'] = train_raw[train_raw['protein_name']=='sEH']['binds'].values\n    train.to_parquet('train_enc.parquet')\n\n    test_raw = pd.read_parquet('/kaggle/input/leash-BELKA/test.parquet')\n    smiles = test_raw['molecule_smiles'].values\n\n    smiles_enc = joblib.Parallel(n_jobs=96)(joblib.delayed(encode_smile)(smile) for smile in tqdm(smiles))\n    smiles_enc = np.stack(smiles_enc)\n    test = pd.DataFrame(smiles_enc, columns=[f'enc{i}' for i in range(142)])\n    test.to_parquet('test_enc.parquet')\n\nelse:\n    train = pd.read_parquet('/kaggle/input/belka-enc-dataset/train_enc.parquet')\n    test = pd.read_parquet('/kaggle/input/belka-enc-dataset/test_enc.parquet')\n\n# Validation function to check encoding values\ndef validate_encodings(data, max_value=35):\n    invalid_indices = np.where(data > max_value)\n    if len(invalid_indices[0]) > 0:\n        print(f\"Invalid encoding values found at indices: {invalid_indices}\")\n        data[invalid_indices] = 0  # Replace invalid values with a valid index (e.g., 0)\n    return data\n\ntrain_encodings = train.drop(columns=['bind1', 'bind2', 'bind3']).values\ntest_encodings = test.values\n\n# Validate and correct encodings\ntrain_encodings = validate_encodings(train_encodings)\ntest_encodings = validate_encodings(test_encodings)\n\n# 1D-CNN model\ndef OneDCNN_model():\n    INP_LEN = 142\n    NUM_FILTERS = 32\n    hidden_dim = 128\n\n    inputs = tf.keras.layers.Input(shape=(INP_LEN,), dtype='int32')\n    x = tf.keras.layers.Embedding(input_dim=36, output_dim=hidden_dim, input_length=INP_LEN, mask_zero=True)(inputs)\n    x = tf.keras.layers.Conv1D(filters=NUM_FILTERS, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n    x = tf.keras.layers.Conv1D(filters=NUM_FILTERS*2, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n    x = tf.keras.layers.Conv1D(filters=NUM_FILTERS*3, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n    x = tf.keras.layers.GlobalMaxPooling1D()(x)\n\n    x = tf.keras.layers.Dense(1024, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.1)(x)\n    x = tf.keras.layers.Dense(1024, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.1)(x)\n    x = tf.keras.layers.Dense(512, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.1)(x)\n\n    outputs = tf.keras.layers.Dense(3, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=CFG.LR, weight_decay=CFG.WD)\n    loss = 'binary_crossentropy'\n    weighted_metrics = [tf.keras.metrics.AUC(curve='PR', name='avg_precision')]\n    model.compile(\n        loss=loss,\n        optimizer=optimizer,\n        weighted_metrics=weighted_metrics,\n    )\n    return model\n\nFEATURES = [f'enc{i}' for i in range(142)]\nTARGETS = ['bind1', 'bind2', 'bind3']\n\nskf = StratifiedKFold(n_splits=CFG.NBR_FOLDS, shuffle=True, random_state=CFG.SEED)\n\nif CFG.PRETRAINED:\n    print(\"Model Pretrained. Skip Training process.\")\nelse:\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train[TARGETS].sum(axis=1))):\n        if fold in CFG.SELECTED_FOLDS:\n            print(f\"Working on fold {fold}\")\n            X_train = train_encodings[train_idx]\n            y_train = train[TARGETS].values[train_idx]\n            X_val = train_encodings[valid_idx]\n            y_val = train[TARGETS].values[valid_idx]\n\n            es = tf.keras.callbacks.EarlyStopping(patience=5, monitor=\"val_loss\", mode='min', verbose=1)\n            checkpoint = tf.keras.callbacks.ModelCheckpoint(\n                monitor='val_loss', filepath=f\"model-{fold}.h5\",\n                save_best_only=True, save_weights_only=True, mode='min'\n            )\n            reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(\n                monitor='val_loss', factor=0.05, patience=5, verbose=1\n            )\n            model = OneDCNN_model()\n            history = model.fit(\n                X_train, y_train,\n                validation_data=(X_val, y_val),\n                epochs=CFG.EPOCHS,\n                callbacks=[checkpoint, reduce_lr_loss, es],\n                batch_size=CFG.BATCH_SIZE,\n                verbose=1,\n            )\n\n# Ensemble predictions\nall_preds = []\nfor fold in CFG.EXIST_MODELS:\n    print(f\"Loading weight model-{fold}.h5\")\n    model = OneDCNN_model()\n    model.load_weights(f\"/kaggle/input/1dcnn-models-for-belka-competition/model-{fold}.h5\")\n    preds = model.predict(test_encodings, batch_size=2*CFG.BATCH_SIZE)\n    all_preds.append(preds)\npreds = np.mean(all_preds, axis=0)\n\n# Submission preparation\ntst = pd.read_parquet('/kaggle/input/leash-BELKA/test.parquet')\ntst['binds'] = 0\ntst.loc[tst['protein_name']=='BRD4', 'binds'] = preds[(tst['protein_name']=='BRD4').values, 0]\ntst.loc[tst['protein_name']=='HSA', 'binds'] = preds[(tst['protein_name']=='HSA').values, 1]\ntst.loc[tst['protein_name']=='sEH', 'binds'] = preds[(tst['protein_name']=='sEH').values, 2]\ntst[['id', 'binds']].to_csv('submissionr.csv', index=False)\n\nprint(\"Submission file created: submissionr.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport gc\n\n# Set the device to GPU (if available) or CPU\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {device}')\n\n# Load a tiny subset of the dataset\ndf = pd.read_csv('/kaggle/input/sample-submission/sample_submission.csv')  # Adjusted path\ndf = df.head(25)  # Use only the first n rows for demonstration\n\n# Check for missing values\nprint(df.isnull().sum())\n\n# Drop rows with missing values\ndf = df.dropna()\n\n# Ensure the SMILES column is of type str\ndf.iloc[:, 0] = df.iloc[:, 0].astype(str)\n\n# Check the data types\nprint(df.dtypes)\n\n# Split the data into training and testing sets (in this case, we use the entire dataset for simplicity)\ntrain_df, test_df = df, df\n\n# Create a custom dataset class\nclass BindingAffinityDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        smiles = self.df.iloc[idx, 0]\n        label = self.df.iloc[idx, 1]\n\n        encoding = self.tokenizer.encode_plus(\n            smiles,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n        label_tensor = torch.tensor(label, dtype=torch.float)\n\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': label_tensor\n        }\n\n# Create a tokenizer\ntokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert', do_lower_case=True)\n\n# Reduce the maximum sequence length and batch size\nmax_len = 128\nbatch_size = 2  # Reducing batch size further\n\n# Create data loaders\ntrain_dataset = BindingAffinityDataset(train_df, tokenizer, max_len=max_len)\ntest_dataset = BindingAffinityDataset(test_df, tokenizer, max_len=max_len)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n# Load the ChemBERT model\nmodel = BertForSequenceClassification.from_pretrained('Rostlab/prot_bert', num_labels=1)\n\n# Set the device to GPU (if available) or CPU\nmodel.to(device)\n\n# Define the loss function and optimizer\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n\n# Add early stopping parameters\nearly_stopping_patience = 3\nbest_loss = float('inf')\nearly_stopping_counter = 0\n\n# Train the model for more epochs\nnum_epochs = 5  # Increased number of epochs\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for i, batch in enumerate(train_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        loss = criterion(outputs.logits.squeeze(), labels)\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        # Clear CUDA cache\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    avg_train_loss = total_loss / len(train_loader)\n    print(f'Epoch {epoch+1}, Loss: {avg_train_loss}')\n\n    # Early stopping logic\n    if avg_train_loss < best_loss:\n        best_loss = avg_train_loss\n        early_stopping_counter = 0\n    else:\n        early_stopping_counter += 1\n        if early_stopping_counter >= early_stopping_patience:\n            print(\"Early stopping triggered\")\n            break\n\n# Evaluate the model\nmodel.eval()\ntest_preds = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n\n        # Debug print statements to check shapes\n        print(f'Logits shape: {outputs.logits.shape}')\n\n        preds = outputs.logits.squeeze().cpu().numpy()\n        if preds.ndim == 0:\n            preds = np.expand_dims(preds, 0)\n        \n        test_preds.extend(preds)\n\n# Convert predictions to numpy array\ntest_preds = np.array(test_preds)\n\n# Ensure the number of predictions matches the number of test samples\nassert len(test_preds) == len(test_df), \"Mismatch between predictions and actual data length.\"\n\n# Evaluate the model using additional metrics\nmse = mean_squared_error(test_df.iloc[:, 1], test_preds)\nmae = mean_absolute_error(test_df.iloc[:, 1], test_preds)\nr2 = r2_score(test_df.iloc[:, 1], test_preds)\n\nprint(f'MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}')\n\n# Prepare the output DataFrame\noutput_df = test_df.copy()\noutput_df['Predicted_Binding_Affinity'] = test_preds\n\n# Save the predictions to a CSV file\noutput_file = 'binding_affinity_predictions.csv'\noutput_df.to_csv(output_file, index=False)\n\nprint(f'Predictions saved to {output_file}')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T07:34:48.847331Z","iopub.execute_input":"2024-07-01T07:34:48.847794Z","iopub.status.idle":"2024-07-01T07:43:22.504088Z","shell.execute_reply.started":"2024-07-01T07:34:48.847758Z","shell.execute_reply":"2024-07-01T07:43:22.502684Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Device: cpu\nid       0\nbinds    0\ndtype: int64\nid        object\nbinds    float64\ndtype: object\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/2090314772.py:24: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0     295246830\n1     295246831\n2     295246832\n3     295246833\n4     295246834\n5     295246835\n6     295246836\n7     295246837\n8     295246838\n9     295246839\n10    295246840\n11    295246841\n12    295246842\n13    295246843\n14    295246844\n15    295246845\n16    295246846\n17    295246847\n18    295246848\n19    295246849\n20    295246850\n21    295246851\n22    295246852\n23    295246853\n24    295246854\nName: id, dtype: object' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  df.iloc[:, 0] = df.iloc[:, 0].astype(str)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 0.18251474774800813\nEpoch 2, Loss: 0.020184286827525984\nEpoch 3, Loss: 0.000877663443903797\nEpoch 4, Loss: 0.0001148158241072941\nEpoch 5, Loss: 2.8743966385473657e-05\nLogits shape: torch.Size([2, 1])\nLogits shape: torch.Size([2, 1])\nLogits shape: torch.Size([2, 1])\nLogits shape: torch.Size([2, 1])\nLogits shape: torch.Size([2, 1])\nLogits shape: torch.Size([2, 1])\nLogits shape: torch.Size([2, 1])\nLogits shape: torch.Size([2, 1])\nLogits shape: torch.Size([2, 1])\nLogits shape: torch.Size([2, 1])\nLogits shape: torch.Size([2, 1])\nLogits shape: torch.Size([2, 1])\nLogits shape: torch.Size([1, 1])\nMSE: 0.0000, MAE: 0.0013, R2: 0.0000\nPredictions saved to binding_affinity_predictions.csv\n","output_type":"stream"}]}]}